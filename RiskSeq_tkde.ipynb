{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-19e83623136a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#import matplotlib.pyplot as plt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import scipy.stats\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Aggre(RawData,Time_interval,ave):\n",
    "    # Time_interval表示一个时间间隔的长度（mins）\n",
    "    AggStep = np.int(Time_interval/10)\n",
    "    if RawData.shape[0]> RawData.shape[1]:\n",
    "        Nm_inter = RawData.shape[1]\n",
    "        Total_inter = RawData.shape[0]\n",
    "        Time_intervalNum = np.int(Total_inter/AggStep)\n",
    "        print(\"Time_intervalNum:\",Time_intervalNum)\n",
    "        AggregateData = np.zeros([Time_intervalNum,Nm_inter]) \n",
    "        for i in range(Time_intervalNum):\n",
    "            for k in range(AggStep):\n",
    "                AggregateData[i,:] = AggregateData[i,:]+ RawData[AggStep*i+k,:]        \n",
    "    # 正常情况\n",
    "    else:\n",
    "        Nm_inter = RawData.shape[0]\n",
    "        Total_inter = RawData.shape[1]\n",
    "        Time_intervalNum = np.int(Total_inter/AggStep)\n",
    "        print(\"Time_intervalNum:\",Time_intervalNum)\n",
    "        AggregateData = np.zeros([Nm_inter,Time_intervalNum]) \n",
    "        for i in range(0,Time_intervalNum):\n",
    "            for k in range(0,AggStep):\n",
    "                AggregateData[:,i] = AggregateData[:,i]+ RawData[:,AggStep*i+k]\n",
    "    if ave==False:\n",
    "        return AggregateData\n",
    "    else: \n",
    "        return AggregateData/AggStep  #返回时间步中的平均速度\n",
    "\n",
    "#Acc_30m = Data_Aggre(Acc,Time_interval,ave=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21744帧的情况\n",
    "T_start = 9000\n",
    "T_end = 20000\n",
    "\n",
    "Total_Num = 21744\n",
    "Acc_total = np.load('Dataset/Acc_sum.npy') #5 month 01~05/2017\n",
    "Pickup_total = np.load('Dataset/Taxipick_sum.npy')\n",
    "Dropoff_total = np.load('Dataset/Taxidrop_sum.npy')\n",
    "Taxi_pickd_total = np.load('Dataset/Taxi_pick_diff.npy') \n",
    "Speed_total = np.load('Dataset/Speed_sum_fillFM.npy') \n",
    "Y_coarse = np.load('Dataset/Y_Coarse_Risk.npy')\n",
    "External_total = np.load('Dataset/ex_time_stamp_10min.npy')\n",
    "Static_affinity = np.load('Dataset/static_affinity.npy')\n",
    "\n",
    "\n",
    "AccSUM =np.sum(Acc_total,axis=0)\n",
    "Speed_total = np.where(Speed_total>100,0,Speed_total)\n",
    "Speed_total = np.where(Speed_total>75,75,Speed_total)\n",
    "Speed_total_diff = np.zeros([354, Total_Num])\n",
    "# 选定哪些区域有taxi经过 可以计算交通模式的相似度\n",
    "TaxiRegIndex = list(np.flatnonzero(np.sum(Pickup_total,axis = 1)))\n",
    "for i in range(1,Total_Num):\n",
    "    Speed_total_diff[:,i] = abs(Speed_total[:,i]- Speed_total[:,i-1])\n",
    "\n",
    "    \n",
    "# 对pickup的数据做处理（log化减小差距）\n",
    "Taxi_pickd_total = np.log2(Taxi_pickd_total+2)\n",
    "Pickup_total = np.log2(Pickup_total+2)\n",
    "Speed_total = Speed_total/10\n",
    "# 对speed进行处理\n",
    "Mean_speed = np.mean(Speed_total)\n",
    "Speed_total = np.where(Speed_total==0,Mean_speed,Speed_total)\n",
    "Speed_total_diff = np.log2(Speed_total_diff+2)\n",
    "Acc_prob_negnew = np.load('Dataset/Acc_prob_negnew.npy')\n",
    "Acc_total_new = np.zeros([Acc_total.shape[0],Acc_total.shape[1]])\n",
    "for i in range(354):\n",
    "    Acc_total_new[i,:] = np.where(Acc_total[i,:] == 0, Acc_prob_negnew[i],Acc_total[i,:]*3.5)\n",
    "Acc_total = Acc_total_new/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "def JS_divergence(p,q):\n",
    "    M=(p+q)/2\n",
    "    return 0.5*scipy.stats.entropy(p, M)+0.5*scipy.stats.entropy(q, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cal_Matrix(t):\n",
    "    TrafficPatt = np.zeros([354,7])\n",
    "    for i in range(7):\n",
    "        TrafficPatt[:,i] = Pickup_total[:,t-144*i]\n",
    "    Dynamic_Aff = np.zeros([354,354])\n",
    "    for i in TaxiRegIndex:\n",
    "        for j in TaxiRegIndex:\n",
    "            if np.sum(TrafficPatt[i,:])!=0 and np.sum(TrafficPatt[j,:])!=0:\n",
    "                Dynamic_Aff[i,j]= JS_divergence(TrafficPatt[i,:],TrafficPatt[j,:])\n",
    "                Dynamic_Aff[i,j] = np.exp(-Dynamic_Aff[i,j]/0.35)\n",
    "                Dynamic_Aff[j,i] = Dynamic_Aff[i,j]\n",
    "    Dynamic_Aff = Dynamic_Aff+Static_affinity\n",
    "    \n",
    "#     A = Dynamic_Aff + np.eye(354) #Node_Num\n",
    "#     D_12 = np.diag(np.power(np.array(A.sum(1)), -0.5).flatten(), 0)\n",
    "#     L = A.dot(D_12).transpose().dot(D_12)*10\n",
    "#    return L\n",
    "    return Dynamic_Aff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Construct_inp_out(t,h): # t，h 表示对某一个时刻t之后h个interval进行预测，Acc_Risk, Pick, Speed, Pick_diff, Speed_diff\n",
    "    # organize input sequence  \n",
    "    scale = 10\n",
    "    Acc_D1 = (np.mean(Acc_total[:,t-432:t-288],axis=1)*scale).reshape([354,1])\n",
    "    Acc_D2 = ((Acc_total[:,t-432]+Acc_total[:,t-288])).reshape([354,1])/2\n",
    "    Acc_R6 = Acc_total[:,t-5:t+1].reshape([354,6])\n",
    "    input_seq_Acc = np.concatenate((Acc_D1,Acc_D2,Acc_R6),axis=1)\n",
    "\n",
    "    Pick_D1 = (np.mean(Pickup_total[:,t-432:t-288],axis=1)).reshape([354,1])\n",
    "    Pick_D2 = ((Pickup_total[:,t-432]+Pickup_total[:,t-288])).reshape([354,1])/2\n",
    "    Pick_R6 = Pickup_total[:,t-5:t+1].reshape([354,6])\n",
    "    input_seq_Pick = np.concatenate((Pick_D1,Pick_D2,Pick_R6),axis=1)\n",
    "\n",
    "    Speed_D1 = (np.mean(Speed_total[:,t-432:t-288],axis=1)).reshape([354,1])\n",
    "    Speed_D2 = ((Speed_total[:,t-432]+Speed_total[:,t-288])).reshape([354,1])/2\n",
    "    Speed_R6 = Speed_total[:,t-5:t+1].reshape([354,6])\n",
    "    input_seq_Speed = np.concatenate((Speed_D1,Speed_D2,Speed_R6),axis=1)\n",
    "\n",
    "    Pickdiff_D1= (np.mean(Taxi_pickd_total[:,t-432:t-288],axis=1)).reshape([354,1])\n",
    "    Pickdiff_D2 = ((Taxi_pickd_total[:,t-432]+Taxi_pickd_total[:,t-288])).reshape([354,1])/2\n",
    "    Pickdiff_R6 = Taxi_pickd_total[:,t-5:t+1].reshape([354,6])\n",
    "    input_seq_PickDif = np.concatenate((Pickdiff_D1,Pickdiff_D2,Pickdiff_R6),axis=1)\n",
    "\n",
    "\n",
    "    Speeddf_D1= (np.mean(Speed_total_diff[:,t-432:t-288],axis=1)).reshape([354,1])\n",
    "    Speeddf_D2 = ((Speed_total_diff[:,t-432]+Speed_total_diff[:,t-288])).reshape([354,1])/2\n",
    "    Speeddf_R6 = Speed_total_diff[:,t-5:t+1].reshape([354,6])\n",
    "    input_seq_SpeedDif = np.concatenate((Speeddf_D1,Speeddf_D2,Speeddf_R6),axis=1)\n",
    "    \n",
    "    Ycoarse_D1 = np.mean(Y_coarse[:,t-432:t-288],axis=1).reshape([1,18])\n",
    "    Ycoarse_D2 = ((Y_coarse[:,t-432]+Y_coarse[:,t-288])/2).reshape([1,18])\n",
    "    Ycoarse_R6 = Y_coarse[:,t-5:t+1].transpose()\n",
    "    input_seq_Ycoarse = np.concatenate([Ycoarse_D1,Ycoarse_D2,Ycoarse_R6])\n",
    "    \n",
    "    \n",
    "    Input_Seq = []\n",
    "    Input_Seq.append(input_seq_Acc)\n",
    "    Input_Seq.append(input_seq_Pick)\n",
    "    Input_Seq.append(input_seq_Speed)\n",
    "    Input_Seq.append(input_seq_PickDif)\n",
    "    Input_Seq.append(input_seq_SpeedDif)\n",
    "    Input_Seq = np.array(Input_Seq)\n",
    "\n",
    "\n",
    "    External_D1 = np.mean(External_total[t-432:t-288,:],axis=0).reshape([1,9])\n",
    "    External_D1[0,3] = 0\n",
    "    External_D2 = ((External_total[t-432,:]+External_total[t-288,:])/2).reshape([1,9])\n",
    "    External_R6 = External_total[t-5:t+1,:]\n",
    "    input_Ext = np.concatenate((External_D1,External_D2,External_R6),axis=0)\n",
    "\n",
    "    ##-------------option to hide these codes-------------#\n",
    "    # Matrix_D1 = (Cal_Matrix(t-432)+Cal_Matrix(t-288))/2\n",
    "    # Matrix_D1 = Matrix_D1.reshape([1,354,354])\n",
    "    # Matrix_D2 = Matrix_D1\n",
    "    # Matrix_D2 = Matrix_D2.reshape([1,354,354])\n",
    "    # Matrix_R6 = np.zeros([354,354])\n",
    "    # for i in range(0,6):\n",
    "    #      Matrix_R6 = Matrix_R6 + Cal_Matrix(t-i*144)\n",
    "    # Matrix_R6 = Matrix_R6/6\n",
    "    # Matrix_R6 = Matrix_R6.reshape([1,354,354])\n",
    "    #Matrix_Aff = np.concatenate((Matrix_D1,Matrix_D2,Matrix_R6),axis=0)\n",
    "    ##-------------option to hide these codes-------------#\n",
    "\n",
    "    ##-------------option to hide these codes-------------#\n",
    "    Matrix_D1 = Cal_Matrix(t-3).reshape([1,354,354])\n",
    "    Matrix_Aff = np.concatenate((Matrix_D1,Matrix_D1,Matrix_D1),axis=0)\n",
    "    ##-------------option to hide these codes-------------#\n",
    "    # organize output sequence\n",
    "    Output_Risk = Acc_total[:,t+1:t+h+1].transpose()\n",
    "    Output_Ycoarse = Y_coarse[:,t+1:t+h+1].transpose()\n",
    "    Output_Ext = External_total[t+1:t+h+1,:]\n",
    "    Output_AccSUM = AccSUM[t+1:t+h+1]\n",
    "    return Input_Seq, input_Ext, Matrix_Aff, Output_Risk, input_seq_Ycoarse, Output_Ycoarse,Output_Ext,Output_AccSUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(t1,t2,batch_size):\n",
    "    tt = np.random.choice(range(t1,t2), batch_size)\n",
    "#     tt = np.array([10287, 10289, 10293, 10296, 10298, 10302,\n",
    "#        10304, 10305, 10307, 10309, 10311, 10313, 10315, 10316, 10318,\n",
    "#        10320, 10322, 10323, 10325, 10327, 10329, 10330, 10332, 10334,\n",
    "#        10336, 10338, 10341, 10342, 10343, 10345])\n",
    "    Input_Batch = []\n",
    "    Intput_Ycoarse_B = []\n",
    "    Input_Ext_B = []\n",
    "    Matrix_Aff_B = []\n",
    "    Output_Risk_B = []\n",
    "    Output_Ycoarse_B = []\n",
    "    Output_AccSUM_B = []\n",
    "    Output_Ext_B = []\n",
    "    for i in tt:\n",
    "        Input_Seq, input_Ext, Matrix_Aff, Output_Risk, input_seq_Ycoarse, Output_Ycoarse,Output_Ext,Output_AccSUM = Construct_inp_out(i,6)\n",
    "        Input_Batch.append(Input_Seq)\n",
    "        Input_Ext_B.append(input_Ext)\n",
    "        Matrix_Aff_B.append(Matrix_Aff)\n",
    "        Output_Risk_B.append(Output_Risk)\n",
    "        Output_Ycoarse_B.append(Output_Ycoarse)\n",
    "        Output_Ext_B.append(Output_Ext)\n",
    "        Intput_Ycoarse_B.append(input_seq_Ycoarse)\n",
    "        Output_AccSUM_B.append(Output_AccSUM)\n",
    "    Input_Batch = np.array(Input_Batch)\n",
    "    Input_Ext_B = np.array(Input_Ext_B)\n",
    "    Matrix_Aff_B = np.array(Matrix_Aff_B)\n",
    "    Output_Risk_B = np.array(Output_Risk_B)\n",
    "    Intput_Ycoarse_B = np.array(Intput_Ycoarse_B)\n",
    "    Output_Ycoarse_B = np.array(Output_Ycoarse_B)\n",
    "    Output_Ext_B = np.array(Output_Ext_B)\n",
    "    Output_AccSUM_B = np.array(Output_AccSUM_B)\n",
    "    return Input_Batch,Input_Ext_B,Matrix_Aff_B,Output_Risk_B,Intput_Ycoarse_B,Output_Ycoarse_B,Output_Ext_B,Output_AccSUM_B,tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(N_Ad,SL_Ad,Gra_Adj,HF_A,HC_A, T1,T2,T3,feed_previous = False,reuse_variables=False):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    N_num_Adj = [64,128,192,256,320,512]  #6  default = 1 N_Ad = 3, SL_Ad = 1,Gra_Adj = 2, HF_A = 2,HC_A =2,T1 = 2, T2 = 2, T3 = 2\n",
    "    num_stacked_layers_Adj = [1,2,3,4]    #4  default = 1 SL_Ad = 1\n",
    "    GRADIENT_CLIPPING_Adj = [1.5,2.0,2.5,3.0] #4  default = 2 Gra_Adj = 2\n",
    "    H_F_Adj = [128,192,256,320]           #4  default = 2 HF_A = 2 \n",
    "    H_C_Adj = [9,16,32,48,64]                #4  default = 2 HC_A =2\n",
    "    Lamda_Adj = [0.5,0.8,1,1.2,1.5]       #5 default = 2, #default = 2,#default = 2  T1 = 2, T2 = 2, T3 = 2\n",
    "    Layer_GCN = 6\n",
    "    N_num = N_num_Adj[N_Ad]\n",
    "    Ex_num = 9\n",
    "    Attribute_Num = 5\n",
    "    Y_coarse_dim = 18\n",
    "     ## Network Parameters\n",
    "    # length of input signals\n",
    "    input_seq_len = 8 \n",
    "    # length of output signals\n",
    "    output_seq_len = 6 \n",
    "    # num of stacked lstm layers \n",
    "    num_stacked_layers = num_stacked_layers_Adj[SL_Ad]\n",
    "    # gradient clipping - to avoid gradient exploding\n",
    "    GRADIENT_CLIPPING = GRADIENT_CLIPPING_Adj[Gra_Adj] \n",
    "    # input size 表示输入序列的长度，比如输入序列长度是20，输出12，就是利用前20个连续序列来预测第21-32个值 \n",
    "    # size of LSTM Cell\n",
    "    hidden_dim_F = H_F_Adj[HF_A]\n",
    "    hidden_dim_C = H_C_Adj[HC_A]\n",
    "    # num of input signals\n",
    "    input_dim_F = 354\n",
    "    input_dim_C = 18\n",
    "    # num of output signals\n",
    "    output_dim_F = 354\n",
    "    output_dim_C = 18\n",
    "    # Multi-task weight\n",
    "    lamda_1 = Lamda_Adj[T1]\n",
    "    lamda_2 = Lamda_Adj[T2]\n",
    "    lamda_3 = Lamda_Adj[T3]\n",
    "    # Learning_rate\n",
    "    learning_rate = 0.001\n",
    "    # Regularization rate\n",
    "    lambda_l2_reg = 0.0001 \n",
    "    \n",
    "    #with tf.variable_scope('Seq2seq'):\n",
    "        # Encoder: inputs\n",
    "    def weight_variable(shape):\n",
    "        initial=tf.truncated_normal(shape,stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "        initial=tf.truncated_normal(shape,stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    #with tf.variable_scope('GCN'):\n",
    "    #-----------输入input———————————————#\n",
    "    # 5 (Attribute)    //      8 interval = 2 (expected) + 6(continous)\n",
    "    # Acc_Risk, Pick, Speed, Pick_diff, Speed_diff\n",
    "    Input_Seq = tf.placeholder(shape=(None, Attribute_Num, 354, input_seq_len),dtype=tf.float32) \n",
    "    Input_Ext = tf.placeholder(shape=(None, input_seq_len, Ex_num),dtype=tf.float32)\n",
    "    Matrix_Aff = tf.placeholder(shape=(None, 3, 354, 354),dtype=tf.float32)\n",
    "    Output_Seq = tf.placeholder(shape=(None, output_seq_len, 354),dtype=tf.float32)\n",
    "    Ext_O = tf.placeholder(shape=(None, output_seq_len, Ex_num),dtype=tf.float32)\n",
    "    Intput_Ycoarse = tf.placeholder(shape=(None, input_seq_len, Y_coarse_dim),dtype=tf.float32)\n",
    "    Output_Ycoarse = tf.placeholder(shape=(None, output_seq_len, Y_coarse_dim),dtype=tf.float32)\n",
    "    Acc_SumS = tf.placeholder(shape=(None, output_seq_len),dtype=tf.float32)\n",
    "    Acc_Risk = Input_Seq[:,0,:,:]\n",
    "    Pick = Input_Seq[:,1,:,:]\n",
    "    Speed = Input_Seq[:,2,:,:]\n",
    "    Pick_diff = Input_Seq[:,3,:,:]\n",
    "    Speed_diff = Input_Seq[:,4,:,:]\n",
    "    Dynamic_Feature = []\n",
    "    for i in range(8):\n",
    "        Input = tf.concat([tf.expand_dims(Acc_Risk[:,:,i],-1),tf.expand_dims(Pick[:,:,i],-1),tf.expand_dims(Speed[:,:,i],-1),tf.expand_dims(Pick_diff[:,:,i],-1),tf.expand_dims(Speed_diff[:,:,i],-1)],axis=-1)\n",
    "        Dynamic_Feature.append(Input)\n",
    "    Dynamic_Feature = tf.convert_to_tensor(Dynamic_Feature)\n",
    "    def gen_gcn_weights():\n",
    "        gcn_weights = {}\n",
    "\n",
    "\n",
    "        gcn_weights['gcn1'] = tf.Variable(tf.truncated_normal(shape=(Attribute_Num, N_num), mean=0, stddev=0.1))#\n",
    "        gcn_weights['gcn2'] = tf.Variable(tf.truncated_normal(shape=(N_num, N_num), mean=0, stddev=0.1))#\n",
    "        gcn_weights['gcn9'] = tf.Variable(tf.truncated_normal(shape=(N_num, 1), mean=0, stddev=0.1))#\n",
    "        gcn_weights['fusionH'] = tf.Variable(tf.truncated_normal(shape=(output_dim_C + Ex_num, output_dim_C)))#\n",
    "        gcn_weights['fusion2'] = tf.Variable(tf.truncated_normal(shape=(2, 1), mean=0, stddev=0.1))#\n",
    "        gcn_weights['fusion3'] = tf.Variable(tf.truncated_normal(shape=(64, 32), mean=0, stddev=0.1))#\n",
    "        gcn_weights['fusion3'] = tf.Variable(tf.truncated_normal(shape=(64, 32), mean=0, stddev=0.1))#\n",
    "        gcn_weights['embeds'] = tf.Variable(tf.truncated_normal(shape=(6, 6), mean=0, stddev=0.1))\n",
    "        gcn_weights['high_low'] = tf.Variable(tf.truncated_normal(shape=(256, 128), mean=0, stddev=0.1))\n",
    "        gcn_weights['Seq_out_WF'] = tf.Variable(tf.truncated_normal(shape=(hidden_dim_F, output_dim_F), dtype = tf.float32, mean=0, stddev=0.1))#\n",
    "        gcn_weights['Seq_out_bF'] = tf.Variable(tf.truncated_normal(shape=(output_dim_F,), dtype = tf.float32, mean=0, stddev=0.1))#,mean=0, stddev=0.1\n",
    "\n",
    "        gcn_weights['Seq_out_WC'] = tf.Variable(tf.random_normal(shape=(hidden_dim_C, output_dim_C), dtype = tf.float32))\n",
    "        gcn_weights['Seq_out_bC'] = tf.Variable(tf.random_normal(shape=(output_dim_C,), dtype = tf.float32))\n",
    "\n",
    "      #  gcn_weights['Gated_W'] = tf.Variable(tf.truncated_normal(shape=(output_dim,), dtype = tf.float32,mean=0, stddev=0.1))\n",
    "        gcn_weights['Ext_W'] = tf.Variable(tf.truncated_normal(shape=(input_dim_C+Ex_num,output_dim_C), dtype = tf.float32,mean=0, stddev=0.1))#,mean=0, stddev=0.1\n",
    "        gcn_weights['Ext_W_O'] = tf.Variable(tf.truncated_normal(shape=(Ex_num,input_dim_C), dtype = tf.float32,mean=0, stddev=0.1))#,mean=0, stddev=0.1\n",
    "        gcn_weights['Coarse2Fine'] = tf.Variable(tf.truncated_normal(shape=(Y_coarse_dim,354), dtype = tf.float32, mean=0, stddev=0.1))#,mean=0, stddev=0.1\n",
    "        gcn_weights['Fine2Coarse'] = tf.Variable(tf.truncated_normal(shape=(354,Y_coarse_dim), dtype = tf.float32))\n",
    "        gcn_weights['Coarse2Sum_W'] = tf.Variable(tf.random_normal(shape=(Y_coarse_dim,1), dtype = tf.float32))\n",
    "        gcn_weights['Coarse2Sum_b'] = tf.Variable(tf.random_normal(shape=(1,), dtype = tf.float32))        \n",
    "        return gcn_weights\n",
    "    weights=gen_gcn_weights()\n",
    "\n",
    "    def GCN_1(input1,input_external,ex_num,Adj_M):\n",
    "        #embedding\n",
    "        # input1 = tf.map_fn(lambda x: tf.nn.leaky_relu(tf.matmul(x,weights['embeds']),alpha=0.8,name=None), input1)\n",
    "        #定义第一层网络结构 GCN\n",
    "        #layer1_temp = tf.map_fn(lambda x: tf.matmul(Adj_M, x), input1) #对于input1中的每个部分 都和A_tensor左乘\n",
    "        layer1_temp = tf.matmul(Adj_M, input1)\n",
    "        layer_1_output = tf.map_fn(lambda x: tf.matmul(x, weights['gcn1']), layer1_temp)\n",
    "        # print(layer_1_output.shape)\n",
    "        #定义第二层网络结构 GCN\n",
    "        #layer2_temp = tf.map_fn(lambda x: tf.matmul(Adj_M, x), layer_1_output)\n",
    "        layer2_temp = tf.matmul(Adj_M, layer_1_output)\n",
    "        layer_2_output = tf.map_fn(lambda x: tf.matmul(x, weights['gcn2']), layer2_temp)\n",
    "        #  layer_2_output=tf.nn.elu(layer_2_output)\n",
    "        #layer_con2_5 = tf.map_fn(lambda x: tf.matmul(x, weights['fusion3']), layer_2_output)\n",
    "        \n",
    "        layer_2_output=tf.nn.leaky_relu(layer_2_output,alpha=0.8, name=None)    \n",
    "        \n",
    "        \n",
    "        img_shape = [4,354,64]\n",
    "        #Wx_plus_b = tf.Variable(tf.random_normal(img_shape))\n",
    "        axis = list(range(len(img_shape) - 1))\n",
    "        wb_mean, wb_var = tf.nn.moments(layer_2_output, [0,1])\n",
    "        scale = tf.Variable(tf.ones([N_num]))\n",
    "        offset = tf.Variable(tf.zeros([N_num]))\n",
    "        variance_epsilon = 0.001\n",
    "        layer_2_output = tf.nn.batch_normalization(layer_2_output, wb_mean, wb_var, offset, scale, variance_epsilon)\n",
    "        \n",
    "        \n",
    "        #定义第三层网络结构 GCN\n",
    "        #layer3_temp = tf.map_fn(lambda x: tf.matmul(Adj_M, x), layer_2_output)\n",
    "        layer3_temp = tf.matmul(Adj_M, layer_2_output)\n",
    "        layer_3_output = tf.map_fn(lambda x: tf.matmul(x, weights['gcn2']), layer3_temp)\n",
    "\n",
    "\n",
    "        # layer_3_output=tf.nn.leaky_relu(layer_3_output,alpha=0.8, name=None)\n",
    "        #定义第四层网络结构 GCN\n",
    "        #layer4_temp = tf.map_fn(lambda x: tf.matmul(Adj_M, x), layer_3_output)\n",
    "        layer4_temp = tf.matmul(Adj_M, layer_3_output)\n",
    "        layer_4_output = tf.map_fn(lambda x: tf.matmul(x, weights['gcn2']), layer4_temp)      \n",
    "        \n",
    "        layer_4_output = tf.nn.leaky_relu(layer_4_output,alpha=0.8, name=None)\n",
    "        \n",
    "        \n",
    "        wb_mean, wb_var = tf.nn.moments(layer_4_output, [0,1])\n",
    "        scale = tf.Variable(tf.ones([N_num]))\n",
    "        offset = tf.Variable(tf.zeros([N_num]))\n",
    "        layer_4_output = tf.nn.batch_normalization(layer_4_output, wb_mean, wb_var, offset, scale, variance_epsilon)\n",
    "        \n",
    "        # 先做BN再做Leaky_ReLU\n",
    "        \n",
    "        layer5_temp = tf.matmul(Adj_M, layer_4_output)\n",
    "        layer_5_output = tf.map_fn(lambda x: tf.matmul(x, weights['gcn2']), layer5_temp)\n",
    "        \n",
    "        Layer5ComBine = tf.add(layer_2_output,layer_4_output)  \n",
    "        Layer5ComBine = tf.add(Layer5ComBine,layer_5_output) \n",
    "\n",
    "        \n",
    "        layer6_temp = tf.matmul(Adj_M, Layer5ComBine)\n",
    "        layer_6_output = tf.map_fn(lambda x: tf.matmul(x, weights['gcn9']), layer6_temp)\n",
    "        \n",
    "        wb_mean, wb_var = tf.nn.moments(layer_6_output, [0,1])\n",
    "        scale = tf.Variable(tf.ones([1]))\n",
    "        offset = tf.Variable(tf.zeros([1]))\n",
    "        layer_6_output = tf.nn.batch_normalization(layer_6_output, wb_mean, wb_var, offset, scale, variance_epsilon)\n",
    "        layer_6_output = tf.nn.leaky_relu(layer_6_output,alpha=0.8, name=None)       \n",
    "        \n",
    "        \n",
    "        output= tf.squeeze(layer_6_output , -1) \n",
    "        keep_prob=0.5\n",
    "        External_W = weight_variable([Ex_num,354])  \n",
    "        External_b = bias_variable([354])\n",
    "        External_out = tf.nn.leaky_relu(tf.matmul(input_external,External_W)+External_b,alpha=0.8, name=None)\n",
    "       # External_out = tf.matmul(input_external,External_W)+External_b\n",
    "        output=output + External_out\n",
    "\n",
    "        return output\n",
    "\n",
    "    #----------------------------Risk-Guided LSTMs -----(Seq2Seq Model)--------------------------#\n",
    "    # 细粒度风险feature map的输入序列和输出序列 （F）\n",
    "    Input_Seq_F = []\n",
    "    for i in range(8):\n",
    "        if i<=1:\n",
    "            Input_Seq_F.append(GCN_1(Dynamic_Feature[i],Input_Ext[:,i],9,Matrix_Aff[:,i]))\n",
    "        if i>=2:\n",
    "            Input_Seq_F.append(GCN_1(Dynamic_Feature[i],Input_Ext[:,i],9,Matrix_Aff[:,2])) \n",
    "\n",
    "    Output_Seq_F = []  # 6*354\n",
    "    for i in range(6):\n",
    "        Output_Seq_F.append(Output_Seq[:,i,:])            \n",
    "\n",
    "\n",
    "    # 粗粒度风险feature map的输入和输出（C）\n",
    "    Input_Seq_C = []\n",
    "    for i in range(8):\n",
    "        Input_Seq_C.append(Intput_Ycoarse[:,i,:])\n",
    "    Output_Seq_C = [] # 6*18\n",
    "    for i in range(6):\n",
    "        Output_Seq_C.append(Output_Ycoarse[:,i,:])\n",
    "\n",
    "\n",
    "    # Decoder: target outputs  \n",
    "    # 细粒度事故风险目标输出  \n",
    "    target_seq_F = [\n",
    "        tf.placeholder(tf.float32, shape=(None, output_dim_F), name=\"y\".format(t))\n",
    "          for t in range(output_seq_len)\n",
    "    ]\n",
    "\n",
    "    #  粗粒度事故风险目标输出\n",
    "    target_seq_C = [\n",
    "        tf.placeholder(tf.float32, shape=(None, output_dim_C), name=\"y\".format(t))\n",
    "          for t in range(output_seq_len)\n",
    "    ]\n",
    "    target_seq_F = Output_Seq_F\n",
    "    target_seq_C = Output_Seq_C\n",
    "\n",
    "\n",
    "    # Give a \"GO\" token to the decoder. \n",
    "    # If dec_inp are fed into decoder as inputs, this is 'guided' training; otherwise only the \n",
    "    # first element will be fed as decoder input which is then 'un-guided'\n",
    "    # 这里有个target_seq的shape\n",
    "    # 细粒度的decoder input\n",
    "    dec_inp_F = [ tf.zeros_like(target_seq_F[0], dtype=tf.float32, name=\"GO\") ] + target_seq_F[:-1]\n",
    "\n",
    "    # 粗粒度的decoder input  加context factors\n",
    "    dec_inp_C = [ tf.zeros_like(target_seq_C[0], dtype=tf.float32, name=\"GO\") ] + target_seq_C[:-1]\n",
    "    print(\"dec_inp_C:\",dec_inp_C)\n",
    "\n",
    "    dec_inp_extC = [tf.matmul(Ext_O[:,k,:],weights['Ext_W_O']) for k in range(output_seq_len)]\n",
    "    dec_inp_extC = [ _i+_j  for _i,_j in zip(dec_inp_extC,dec_inp_C)]\n",
    "#     dec_inp_extC = [tf.concat([Ext_O[:,k,:],dec_inp_C[k]],axis = -1) for k in range(output_seq_len)]\n",
    "#     dec_inp_extC = [tf.matmul(dec_inp_extC[k], weights['fusionH']) for k in range(output_seq_len)] \n",
    "#    dec_inp_extC = [ _i+_j  for _i,_j in zip(dec_inp_extC,dec_inp_C)]\n",
    "    print(\"dec_inp_extC:\",dec_inp_extC)\n",
    "\n",
    "    # 得到Accident的总数\n",
    "    Acc_SumOut = []\n",
    "    for i in range(6):\n",
    "        Acc_SumOut.append(Acc_SumS[:,i])\n",
    "\n",
    "\n",
    "    # -------------------此处可以加新的context_factors到dec_inp: concatenate----------------------------#\n",
    "\n",
    "\n",
    "    def _rnn_decoder(decoder_inputs,\n",
    "                    initial_state,\n",
    "                    cell,\n",
    "                    loop_function=None,\n",
    "                    scope=None):\n",
    "\n",
    "        state = initial_state\n",
    "        outputs = []\n",
    "        prev = None\n",
    "        for i, inp in enumerate(decoder_inputs):\n",
    "          if loop_function is not None and prev is not None:\n",
    "            with tf.variable_scope(\"loop_function\", reuse=True):\n",
    "              inp = loop_function(prev, i)\n",
    "          if i > 0:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "          output, state = cell(inp, state)\n",
    "          outputs.append(output)\n",
    "          if loop_function is not None:\n",
    "            prev = output\n",
    "        return outputs, state\n",
    "\n",
    "    def _loop_function(prev, _):\n",
    "        #print(prev)\n",
    "        if prev.shape[1] == hidden_dim_C:\n",
    "            return tf.nn.relu(tf.matmul(prev, weights['Seq_out_WC']) + weights['Seq_out_bC'])\n",
    "        if prev.shape[1] == hidden_dim_F:\n",
    "            return tf.nn.relu(tf.matmul(prev, weights['Seq_out_WF']) + weights['Seq_out_bF'])\n",
    "\n",
    "    def _basic_rnn_seq2seq(encoder_inputs,\n",
    "                      decoder_inputs,\n",
    "                      cell,\n",
    "                      feed_previous,\n",
    "                      dtype=tf.float32,\n",
    "                      scope=None):\n",
    "        # Encoder  \n",
    "        enc_cell = cell\n",
    "        _, enc_state = tf.contrib.rnn.static_rnn(enc_cell, encoder_inputs, dtype=dtype)\n",
    "        # Decoder \n",
    "        if feed_previous:\n",
    "            return _rnn_decoder(decoder_inputs, enc_state, cell, _loop_function)\n",
    "        else:\n",
    "            return _rnn_decoder(decoder_inputs, enc_state, cell)\n",
    "\n",
    "\n",
    "\n",
    "    def _rnn_decoder(decoder_inputs,\n",
    "                        initial_state,\n",
    "                        cell,\n",
    "                        loop_function=None,\n",
    "                        scope=None):\n",
    "\n",
    "            state = initial_state\n",
    "            outputs = []\n",
    "            prev = None\n",
    "            for i, inp in enumerate(decoder_inputs):\n",
    "              if loop_function is not None and prev is not None:\n",
    "                with tf.variable_scope(\"loop_function\", reuse=True):\n",
    "                  inp = loop_function(prev, i)\n",
    "              if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "              output, state = cell(inp, state)\n",
    "              outputs.append(output)\n",
    "              if loop_function is not None:\n",
    "                prev = output\n",
    "            return outputs, state\n",
    "\n",
    "\n",
    "\n",
    "    def _basic_rnn_seq2seq2(encoder_inputs,\n",
    "                      decoder_inputs,\n",
    "                      cell,\n",
    "                      feed_previous,\n",
    "                      dtype=tf.float32,\n",
    "                      scope=None):\n",
    "        # Encoder  \n",
    "        enc_cell = cell\n",
    "        _, enc_state = tf.contrib.rnn.static_rnn(enc_cell, encoder_inputs, dtype=dtype)\n",
    "        # Decoder \n",
    "        if feed_previous:\n",
    "            return _rnn_decoder(decoder_inputs, enc_state, cell, _loop_function)\n",
    "        else:\n",
    "            return _rnn_decoder(decoder_inputs, enc_state, cell)\n",
    "\n",
    "\n",
    "\n",
    "    ## 设置LSTM的cell\n",
    "    with tf.variable_scope(\"Fine_RNN\"):  \n",
    "        cells_F = []\n",
    "        for i in range(num_stacked_layers):\n",
    "            with tf.variable_scope('RNN_{}'.format(i)):\n",
    "                cells_F.append(tf.contrib.rnn.LSTMCell(num_units = hidden_dim_F, activation = tf.nn.leaky_relu)) # activation = tf.nn.leaky_relu\n",
    "        cell_F = tf.contrib.rnn.MultiRNNCell(cells_F)\n",
    "        ## 把粗粒度和细粒度分别喂到LSTM中\n",
    "    #\n",
    "        dec_outputs_F, dec_memory_F = _basic_rnn_seq2seq(\n",
    "        Input_Seq_F, \n",
    "        dec_inp_F,  ##输入带有context的数据\n",
    "        cell_F, \n",
    "        feed_previous = feed_previous\n",
    "        )   \n",
    "    with tf.variable_scope(\"Coarse_RNN\"):  \n",
    "        cells_C = []\n",
    "        for i in range(num_stacked_layers):\n",
    "            with tf.variable_scope('RNN_{}'.format(i)):\n",
    "                cells_C.append(tf.contrib.rnn.LSTMCell(num_units = hidden_dim_C, activation = tf.nn.relu)) # activation = tf.nn.leaky_relu\n",
    "        cell_C = tf.contrib.rnn.MultiRNNCell(cells_C)\n",
    "        print(\"111\")\n",
    "        dec_outputs_C, dec_memory_C = _basic_rnn_seq2seq(\n",
    "        Input_Seq_C, \n",
    "        dec_inp_extC,  ##输入带有context的数据\n",
    "        cell_C, \n",
    "        feed_previous = feed_previous\n",
    "        )\n",
    "    print(\"target_seq_C\",target_seq_C[:-1])\n",
    "\n",
    "    #print(\"basic:\",tf.get_variable_scope().reuse)# basic_rnn_seq2seq会将reuse改变成resuse=True\n",
    "\n",
    "\n",
    "    reshaped_outputsF = [tf.nn.leaky_relu(tf.matmul(i, weights['Seq_out_WF']) + weights['Seq_out_bF'],alpha=0.8) for i in dec_outputs_F]\n",
    "\n",
    "    reshaped_outputsC = [tf.nn.relu(tf.matmul(i, weights['Seq_out_WC'])+ weights['Seq_out_bC']) for i in dec_outputs_C]\n",
    "\n",
    "    reshaped_outputsF = [tf.nn.leaky_relu(tf.matmul(i, weights['Coarse2Fine']) + j,alpha=0.8) for i,j in zip(reshaped_outputsC,reshaped_outputsF)]\n",
    "\n",
    "\n",
    "    SUM_outputs = [tf.nn.relu(tf.matmul(i, weights['Coarse2Sum_W']) + weights['Coarse2Sum_b']) for i in reshaped_outputsC]\n",
    "\n",
    "\n",
    "    output_loss = 0\n",
    "    output_lossT = 0\n",
    "    output_lossC = 0\n",
    "    output_lossF = 0\n",
    "    reg_loss = 0\n",
    "\n",
    "    for _y, _Y in zip(SUM_outputs, Acc_SumOut):\n",
    "        output_lossT += tf.reduce_mean(tf.pow(_y - _Y, 2))\n",
    "\n",
    "    for _y, _Y in zip(reshaped_outputsC, Output_Seq_C):\n",
    "        output_lossC += tf.reduce_mean(tf.pow(_y - _Y, 2))\n",
    "\n",
    "    for _y, _Y in zip(reshaped_outputsF, Output_Seq_F):\n",
    "        output_lossF += tf.reduce_mean(tf.pow(_y - _Y, 2))\n",
    "\n",
    "    for tf_var in tf.trainable_variables():\n",
    "        if 'Seq_out_' in tf_var.name in tf_var.name:\n",
    "            reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "    # L2 regularization for weights and biases\n",
    "\n",
    "    #  reg = tf.contrifb.layers.apply_regularization(tf.contrib.layers.l2_regularizer(1e-4),tf.trainable_variables())\n",
    "\n",
    "    loss = lamda_1 * output_lossT + lamda_2 * output_lossC + lamda_3 * output_lossF + lambda_l2_reg * reg_loss\n",
    "    global_step = tf.Variable(\n",
    "              initial_value=0,\n",
    "             # name=\"global_step\",\n",
    "              trainable=False,\n",
    "              collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "    #print(tf.get_variable_scope().reuse)\n",
    "    starter_learning_rate = learning_rate\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 10, 0.75, staircase=True)\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "    #         optimizer = tf.contrib.layers.optimize_loss(\n",
    "    #             loss=loss,\n",
    "    #             learning_rate=learning_rate,\n",
    "    #             global_step=global_step,\n",
    "    #             optimizer='Adam',\n",
    "    #             clip_gradients=GRADIENT_CLIPPING)\n",
    "        # global_step=global_step,\n",
    "        \n",
    "        optimizer = tf.contrib.layers.optimize_loss(\n",
    "        loss=loss,\n",
    "        learning_rate = learning_rate,\n",
    "        global_step=global_step,\n",
    "        optimizer='Adam',\n",
    "        clip_gradients=GRADIENT_CLIPPING)\n",
    "\n",
    "    saver = tf.train.Saver\n",
    "    print(SUM_outputs)\n",
    "    Hyper_param = dict(\n",
    "        Layer_GCN = Layer_GCN,\n",
    "        N_num = N_num,\n",
    "        Ex_num = Ex_num,\n",
    "        # num of stacked lstm layers \n",
    "        num_stacked_layers = num_stacked_layers,\n",
    "        # gradient clipping - to avoid gradient exploding\n",
    "        GRADIENT_CLIPPING = GRADIENT_CLIPPING,\n",
    "        # size of LSTM Cell\n",
    "        hidden_dim_F = hidden_dim_F,\n",
    "        hidden_dim_C = hidden_dim_C,\n",
    "        lambda_l2_reg = lambda_l2_reg,\n",
    "        learning_rate = learning_rate,\n",
    "        lamda_1 = lamda_1,\n",
    "        lamda_2 = lamda_2,\n",
    "        lamda_3 = lamda_3\n",
    "        \n",
    "    )\n",
    "    return dict(\n",
    "        Input_Seq = Input_Seq,\n",
    "        Output_Seq = Output_Seq,\n",
    "        Input_Ext = Input_Ext, \n",
    "        Matrix_Aff = Matrix_Aff,\n",
    "        Ext_O = Ext_O,\n",
    "        Output_Ycoarse = Output_Ycoarse,\n",
    "        train_op = optimizer, \n",
    "        loss=loss,\n",
    "        saver = saver, \n",
    "        Acc_SumS = Acc_SumS,\n",
    "        reshaped_outputsF = reshaped_outputsF,\n",
    "        reshaped_outputsC = reshaped_outputsC,\n",
    "        Intput_Ycoarse = Intput_Ycoarse,\n",
    "        SUM_outputs = SUM_outputs,\n",
    "        Hyper_param = Hyper_param\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = build_graph(N_Ad = 4, SL_Ad = 1,Gra_Adj = 2, HF_A = 3,HC_A =2,T1 = 3, T2 = 1, T3 = 2,feed_previous=False)\n",
    "#     N_num_Adj = [64,128,192,256,320,512]  #6  default = 3 N_Ad = 3, SL_Ad = 1,Gra_Adj = 2, HF_A = 2,HC_A =2,T1 = 2, T2 = 2, T3 = 2\n",
    "#     num_stacked_layers_Adj = [1,2,3,4]    #4  default = 1 SL_Ad = 1\n",
    "#     GRADIENT_CLIPPING_Adj = [1.5,2,2.5,3] #4  default = 2 Gra_Adj = 2\n",
    "#     H_F_Adj = [128,192,256,320]           #4  default = 2 HF_A = 2 \n",
    "#     H_C_Adj = [9,16,32,64]                #4  default = 2 HC_A =2\n",
    "#     Lamda_Adj = [0.5,0.8,1,1.2,1.5] (TCF) #5 default = 2, #default = 2,#default = 2  T1 = 2, T2 = 2, T3 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接进行训练，需训练到35个epoch之后能达到50\\%的命中率\n",
    "import csv\n",
    "#import datetime\n",
    "#import evaluation\n",
    "#rnn_model = build_graph(N_Ad = 3, SL_Ad = 1,Gra_Adj = 2, HF_A = 2,HC_A =2,T1 = 2, T2 = 2, T3 = 2,feed_previous=False)\n",
    "saver = tf.train.Saver()\n",
    "epoch_loss = []\n",
    "# starttime = datetime.datetime.now()\n",
    "init = tf.global_variables_initializer()\n",
    "K = 30\n",
    "epoch =100\n",
    "Batchsize = 30\n",
    "Output_seq_len = 6\n",
    "Total_sample = 10000-9000\n",
    "\n",
    "# print(Total_sample,Batchsize,Batchsize*2,Batch_Num)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    max_Acc = 0\n",
    "    Total_loss = 0\n",
    "    Total_Accarcy = 0\n",
    "    for i in range(epoch): \n",
    "        sub_loss_sum = 0\n",
    "        final_preds_F = []\n",
    "        Acc_Real = np.zeros([Output_seq_len])\n",
    "        Acc_Iden = np.zeros([Output_seq_len])\n",
    "        Input_Batch,Input_Ext_B,Matrix_Aff_B,Output_Risk_B,Input_Ycoarse_B, Output_Ycoarse_B,Output_Ext_B,SUMOut,SelectedTime = generate_samples(2500,4500,Batchsize)\n",
    "        feed_dict = {rnn_model['Input_Seq']:Input_Batch, rnn_model['Input_Ext']:Input_Ext_B ,rnn_model['Matrix_Aff']:Matrix_Aff_B,rnn_model['Output_Seq']:Output_Risk_B,\n",
    "                    rnn_model['Intput_Ycoarse']:Input_Ycoarse_B, rnn_model['Ext_O']:Output_Ext_B, rnn_model['Output_Ycoarse']: Output_Ycoarse_B,rnn_model['Acc_SumS']:SUMOut}\n",
    "        _, loss_t,final_preds_f = sess.run([rnn_model['train_op'], rnn_model['loss'],rnn_model['reshaped_outputsF']], feed_dict)\n",
    "        print(i,loss_t)\n",
    "        final_preds_F.append(final_preds_f)\n",
    "        final_predsF = np.array(final_preds_F)  \n",
    "        Total_Acc = 0\n",
    "        Pred_Acc = 0\n",
    "        for batch in range(Batchsize):\n",
    "            for seq_step in range(Output_seq_len):\n",
    "                Real = list(np.flatnonzero((Output_Risk_B[batch,seq_step,:])>0)) # 10,6,354 batch_size,time_step,output_dim\n",
    "                Predicted = list(np.argsort(-final_predsF[0,seq_step,batch,:])[0:K]) # 1,6,10,354  1,time_step,batch_size,output_dim\n",
    "                Total_Acc = Total_Acc + len(Real)        \n",
    "                Cross = list(set(Predicted).intersection(set(Real)))\n",
    "                # print(Cross)\n",
    "                Pred_Acc = Pred_Acc + len(Cross)\n",
    "                Acc_Real[seq_step] = Acc_Real[seq_step] + len(Real)\n",
    "                Acc_Iden[seq_step] = Acc_Iden[seq_step] + len(Cross)\n",
    "        Total_Accarcy = Pred_Acc/Total_Acc\n",
    "        print(Total_Acc,Pred_Acc,Pred_Acc/Total_Acc,Total_loss)\n",
    "       \n",
    "        if (Total_Accarcy> 0.5) & (np.flatnonzero(np.isnan(Acc_Real)).shape[0]==0): \n",
    "            print(Acc_Iden/Acc_Real)\n",
    "\n",
    "        if Total_Accarcy> max_Acc:\n",
    "            max_Acc = Pred_Acc/Total_Acc\n",
    "            temp_saver = rnn_model['saver']()\n",
    "    temp_saver = rnn_model['saver']()\n",
    "    txt_file = \"Hyper_param.txt\"\n",
    "    with open(txt_file,\"a+\", newline='') as ofile:\n",
    "        field_names = ['Layer_GCN', 'N_num', 'Ex_num','num_stacked_layers','GRADIENT_CLIPPING', 'hidden_dim_F', 'hidden_dim_C', 'lambda_l2_reg',\n",
    "        'learning_rate','lamda_1','lamda_2', 'lamda_3','Acc']\n",
    "\n",
    "        odict = csv.DictWriter(ofile, field_names)\n",
    "        row = rnn_model['Hyper_param']\n",
    "        row2 = dict(Acc = max_Acc)\n",
    "        odict.writerow(row)\n",
    "        odict.writerow(row2)\n",
    "        ofile.close()    \n",
    "    save_path = temp_saver.save(sess, os.path.join('./', 'Multi_scaleFore_Results/CGcon_3_1_2_2_MixHop/univariate_testcitywide'))\n",
    "\n",
    "print(\"Checkpoint saved at: \", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集进行测试\n",
    "Batchsize = 30\n",
    "Output_seq_len = 6\n",
    "rnn_model = build_graph(N_Ad = 4, SL_Ad = 1,Gra_Adj = 2, HF_A = 3,HC_A =2,T1 = 3, T2 = 1, T3 = 2,feed_previous=False)\n",
    "init = tf.global_variables_initializer()\n",
    "t1 = 9000\n",
    "t2 = 20000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    final_preds_F = []\n",
    "    final_preds_C = []\n",
    "    Sum_Pred = []\n",
    "    sess.run(init)\n",
    "    saver = rnn_model['saver']().restore(sess, os.path.join('./', 'Multi_scaleFore_Results/CGcon_3_1_2_2_MixHop/univariate_testcitywide'))\n",
    "    Test_Input_Batch,Test_Input_Ext_B,Test_Matrix_Aff_B,Test_Output_Risk_B,Test_Input_Ycoarse_B, Test_Output_Ycoarse_B,Test_Output_Ext_B,Test_SUM,SelectedTime = generate_samples(T_start,T_end,Batchsize)\n",
    "    feed_dict = {rnn_model['Input_Seq']: Test_Input_Batch,rnn_model['Input_Ext']:Test_Input_Ext_B,rnn_model['Matrix_Aff']:Test_Matrix_Aff_B,rnn_model['Output_Seq']:Test_Output_Risk_B,\n",
    "                rnn_model['Intput_Ycoarse']:Test_Input_Ycoarse_B, rnn_model['Ext_O']:Test_Output_Ext_B, rnn_model['Output_Ycoarse']: Test_Output_Ycoarse_B,rnn_model['Acc_SumS']:Test_SUM\n",
    "                }          \n",
    "    final_preds_f,final_preds_c,sum_outputs = sess.run([rnn_model['reshaped_outputsF'],rnn_model['reshaped_outputsC'],rnn_model['SUM_outputs']],feed_dict)\n",
    "    final_preds_F.append(final_preds_f)\n",
    "    final_preds_C.append(final_preds_c)\n",
    "    Sum_Pred.append(sum_outputs)\n",
    "final_preds_F = np.array(final_preds_F)\n",
    "final_preds_C = np.array(final_preds_C)\n",
    "Sum_Pred = np.array(Sum_Pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing process\n",
    "K = 20\n",
    "Total_Acc = 0\n",
    "Pred_Acc = 0\n",
    "Acc_Iden = np.zeros([6])\n",
    "Acc_Real = np.zeros([6])\n",
    "FError = 0\n",
    "CError = 0\n",
    "FMAPE = 0\n",
    "CMAPE = 0\n",
    "for batch in range(Batchsize):\n",
    "    for seq_step in range(Output_seq_len):\n",
    "        #batch = 5\n",
    "        #seq_step = 0\n",
    "        Real = list(np.flatnonzero((Test_Output_Risk_B[batch,seq_step,:])>0)) # 10,6,354 batch_size,time_step,output_dim\n",
    "        # K = np.int(np.sum(final_preds_C[0,seq_step,batch,:]))\n",
    "        K = 20\n",
    "        # K = np.int(np.round(Sum_Pred[0,seq_step,batch,0]))+ 8 #Sum_Pred[0,seq_step,batch,0]\n",
    "        # Predicted = Risk_Proposal(final_preds_F[0,seq_step,batch,:],final_preds_C[0,seq_step,batch,:])\n",
    "        Predicted = list(np.argsort(-final_preds_F[0,seq_step,batch,:])[0:K]) # 1,6,10,354  1,time_step,batch_size,output_dim\n",
    "        Total_Acc = Total_Acc + len(Real)        \n",
    "        Cross = list(set(Predicted).intersection(set(Real)))\n",
    "        if len(Cross) == 0:\n",
    "            print(11)\n",
    "            continue\n",
    "        #print(Cross)\n",
    "        Pred_Acc = Pred_Acc + len(Cross)\n",
    "        Acc_single = len(Cross)/len(Real)\n",
    "        print(Acc_single)\n",
    "        Acc_Real[seq_step] = Acc_Real[seq_step] + len(Real)\n",
    "        Acc_Iden[seq_step] = Acc_Iden[seq_step] + len(Cross)\n",
    "        # MSE/MAPE\n",
    "        F_error = np.mean((final_preds_F[0,seq_step,batch,:]-Test_Output_Risk_B[batch,seq_step,:])**2)\n",
    "        C_error = np.mean((np.int32(final_preds_C[0,seq_step,batch,:])-Test_Output_Ycoarse_B[batch,seq_step,:])**2)\n",
    "        F_mape = np.mean(abs((final_preds_F[0,seq_step,batch,:]-Test_Output_Risk_B[batch,seq_step,:])/Test_Output_Risk_B[batch,seq_step,:]))\n",
    "        Test_Output_Ycoarse_B1 = np.where(Test_Output_Ycoarse_B==0,-1,Test_Output_Ycoarse_B)\n",
    "        final_preds_C1 = np.where(final_preds_C==0,-1,final_preds_C)\n",
    "        C_mape = np.mean(abs((np.int32(final_preds_C1[0,seq_step,batch,:])-Test_Output_Ycoarse_B1[batch,seq_step,:])/Test_Output_Ycoarse_B1[batch,seq_step,:]))\n",
    "        FError = FError + F_error\n",
    "        CError = CError + C_error\n",
    "        FMAPE = FMAPE + F_mape\n",
    "        CMAPE = CMAPE + C_mape\n",
    "FError = FError/(Batchsize*Output_seq_len)\n",
    "CError = CError/(Batchsize*Output_seq_len)\n",
    "FMAPE = FMAPE/(Batchsize*Output_seq_len)\n",
    "CMAPE = CMAPE/(Batchsize*Output_seq_len)\n",
    "print(\"Acc:\",Total_Acc,Pred_Acc,Pred_Acc/Total_Acc)\n",
    "print(\"MSE/(F/C):\",FError,CError,\"MAPE(F/C)\",FMAPE,CMAPE)\n",
    "\n",
    "\n",
    "# Test process\n",
    "# F_error = np.mean((final_preds_F[0,seq_step,batch,:]-Test_Output_Risk_B[batch,seq_step,:])**2)\n",
    "# C_error = np.mean((np.int32(final_preds_C[0,seq_step,batch,:])-Test_Output_Ycoarse_B[batch,seq_step,:])**2)\n",
    "# F_mape = np.mean(abs((final_preds_F[0,seq_step,batch,:]-Test_Output_Risk_B[batch,seq_step,:])/Test_Output_Risk_B[batch,seq_step,:]))\n",
    "# Test_Output_Ycoarse_B1 = np.where(Test_Output_Ycoarse_B==0,-1,Test_Output_Ycoarse_B)\n",
    "# final_preds_C1 = np.where(final_preds_C==0,-1,final_preds_C)\n",
    "# C_mape = np.mean(abs((np.int32(final_preds_C1[0,seq_step,batch,:])-Test_Output_Ycoarse_B1[batch,seq_step,:])/Test_Output_Ycoarse_B1[batch,seq_step,:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其他可能用到的代码（Other codes that can be utilized for deriving results）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "F2C=np.load('Dataset/Cluster_18_6.npy') \n",
    "\n",
    "def Fine2Coarse(FineMap):\n",
    "    Coarse_Y = np.zeros(18,)\n",
    "    for i in range(18):\n",
    "        for k in range(36):\n",
    "            if(F2C[i,k]!=0):\n",
    "                Coarse_Y[i] = Coarse_Y[i] + FineMap[F2C[i,k]] \n",
    "    return Coarse_Y\n",
    "#FineMap1 = np.ones([354,1])\n",
    "Coarse_Y = Fine2Coarse(FineMap1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分时段评估\n",
    "K = 30\n",
    "Total_Acc = 0\n",
    "Pred_Acc = 0\n",
    "Acc_Iden = np.zeros([6])\n",
    "Acc_Real = np.zeros([6])\n",
    "for batch in range(Batchsize):\n",
    "    for seq_step in range(Output_seq_len):\n",
    "        #batch = 5\n",
    "        #seq_step = 0\n",
    "        Real = list(np.flatnonzero((Test_Output_Risk_B[batch,seq_step,:])>0)) # 10,6,354 batch_size,time_step,output_dim\n",
    "        Predicted = Risk_Proposal(final_preds_F[0,seq_step,batch,:],final_preds_C[0,seq_step,batch,:])\n",
    "        # Predicted = list(np.argsort(-final_preds_F[0,seq_step,batch,:])[0:K]) # 1,6,10,354  1,time_step,batch_size,output_dim\n",
    "        Total_Acc = Total_Acc + len(Real)        \n",
    "        Cross = list(set(Predicted).intersection(set(Real)))\n",
    "        print(Cross)\n",
    "        Pred_Acc = Pred_Acc + len(Cross)\n",
    "        Acc_Real[seq_step] = Acc_Real[seq_step] + len(Real)\n",
    "        Acc_Iden[seq_step] = Acc_Iden[seq_step] + len(Cross)\n",
    "print(Total_Acc,Pred_Acc,Pred_Acc/Total_Acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行提名 返回哪些是高风险的区域\n",
    "Cluster_results = np.load('Dataset/Cluster_18_6.npy')\n",
    "\n",
    "def Risk_Proposal(Task1,Task3):\n",
    "    count1=0\n",
    "    count2=0\n",
    "    Count=0\n",
    "    Dim_o3 = 18\n",
    "    A_risk=np.zeros([Cluster_results.shape[0],Cluster_results.shape[1]])\n",
    "    for i in range(Dim_o3):\n",
    "        for j in range(np.flatnonzero(Cluster_results[i]).shape[0]):\n",
    "            if Cluster_results[i,j]!=0:\n",
    "                #print(Cluster_results4[i,j])\n",
    "                A_risk[i,j]=Task1[Cluster_results[i,j]]\n",
    "\n",
    "    risk_index=[]    \n",
    "    for i in range(Dim_o3):\n",
    "        k=np.int16(Task3[i])\n",
    "        #print(\"k:\",k)\n",
    "        if i!=0:\n",
    "            Sort1=np.array(-A_risk[i,0:np.flatnonzero(Cluster_results[i]).shape[0]+1]).argsort()\n",
    "        else: \n",
    "            Sort1=np.array(-A_risk[i,1:np.flatnonzero(Cluster_results[i]).shape[0]+1]).argsort()\n",
    "       # print(Sort1)\n",
    "        if (k>=2):\n",
    "            #print(i,Sort1)\n",
    "            SS=Sort1[0:k]\n",
    "            risk_index.append(list(SS))\n",
    "        if (k<=1): #只要k小于等于1则用这个方法\n",
    "            risk_index.append([Sort1[0],Sort1[1]])\n",
    "#             if np.flatnonzero(Cluster_results[i]).shape[0]>13:\n",
    "#                 risk_index.append([Sort1[0],Sort1[1]])\n",
    "#             else:\n",
    "#                 risk_index.append([-1])\n",
    "    #print(\"index\",risk_index)\n",
    "    #由cluster的索引号映射回原来的ID\n",
    "    Highrisk_ID=[]\n",
    "    for i in range(Dim_o3):\n",
    "        #print(\"risk_index[i]:\",risk_index[i])\n",
    "        if  len(risk_index[i])>2:\n",
    "            for kk in risk_index[i]:\n",
    "                #print(\"kk:\",kk)\n",
    "                Highrisk_ID.append(Cluster_results[i,np.int(kk)])\n",
    "                #print(1,i,np.int(kk))\n",
    "        if len(risk_index[i])<=1:\n",
    "            if risk_index[i][0]!=-1:\n",
    "                Highrisk_ID.append(Cluster_results[i,int(risk_index[i][0])])\n",
    "                #print(Cluster_results[i,int(risk_index[i][0])])\n",
    "    Highrisk_ID = np.unique(Highrisk_ID)    \n",
    "    return list(Highrisk_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
